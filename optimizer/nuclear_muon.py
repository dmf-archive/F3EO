import torch
import torch.distributed as dist


def zeropower_via_newtonschulz5(G, steps: int):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


def nuclear_muon_update(grad, momentum, beta=0.95, ns_steps=5, nesterov=True):
    momentum.lerp_(grad, 1 - beta)
    update = grad.lerp_(momentum, beta) if nesterov else momentum
    if update.ndim == 4: # for the case of conv filters
        update = update.view(update.size(0), -1)
    elif update.ndim == 1:
        # Skip orthogonalization for 1D parameters (biases, gains, etc.)
        return update

    # Standard Muon orthogonalization via Newton-Schulz
    # This maintains the gradient direction stability while parameter prox handles low-rank
    update = zeropower_via_newtonschulz5(update, steps=ns_steps)
    update *= max(1, grad.size(-2) / grad.size(-1))**0.5

    return update


def prox_nuclear_norm(param, lambda_val):
    if param.ndim == 4: # Conv2d weights
        param_2d = param.view(param.size(0), -1)
    elif param.ndim >= 2: # Linear and other 2D params
        param_2d = param
    else: # 1D params (biases)
        return param

    try:
        U, S, Vt = torch.linalg.svd(param_2d, full_matrices=False, driver='gesvd')
        S_thresh = torch.clamp(S - lambda_val, min=0)
        param_prox = U @ torch.diag_embed(S_thresh) @ Vt

        if param.ndim == 4:
            return param_prox.view(param.shape)
        return param_prox
    except:
        return param


class NuclearMuon(torch.optim.Optimizer):
    """
    NuclearMuon - MomentUm Orthogonalized by Nuclear Norm Regularization

    NuclearMuon applies layer-wise nuclear norm proximal operator to parameters while
    maintaining Muon's orthogonal gradient updates. This implements true low-rank
    regularization via soft-thresholding of singular values: σ_i → max(0, σ_i - λ).

    The proximal operator prox_λ||·||_*(W) = U diag(max(0, σ_i - λ)) V^T is applied
    directly to parameters before gradient updates, promoting spectral sparsity
    for better generalization under MDL principles.

    Arguments:
        lr: The learning rate, in units of spectral norm per update.
        weight_decay: The AdamW-style weight decay.
        momentum: The momentum. A value of 0.95 here is usually fine.
        nuclear_lambda: Nuclear norm regularization strength (adaptive: λ_t = λ_0/√t).
    """
    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95, nuclear_lambda=0.01):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nuclear_lambda=nuclear_lambda)
        assert isinstance(params, list) and len(params) >= 1 and isinstance(params[0], torch.nn.Parameter)
        params = sorted(params, key=lambda x: x.size(), reverse=True)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            params = group["params"]
            if len(params) == 0:
                continue
            params_pad = params + [torch.empty_like(params[-1])] * (dist.get_world_size() - len(params) % dist.get_world_size())
            for base_i in range(len(params))[::dist.get_world_size()]:
                if base_i + dist.get_rank() < len(params):
                    p = params[base_i + dist.get_rank()]
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1

                    # Apply layer-wise nuclear norm proximal operator to parameters
                    if p.ndim >= 2:
                        step = state["step"]
                        adaptive_lambda = group["nuclear_lambda"] / (step**0.5)
                        p.data = prox_nuclear_norm(p.data, adaptive_lambda)

                    update = nuclear_muon_update(p.grad, state["momentum_buffer"], beta=group["momentum"])
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update.reshape(p.shape), alpha=-group["lr"])
                dist.all_gather(params_pad[base_i:base_i + dist.get_world_size()], params_pad[base_i + dist.get_rank()])

        return loss


class SingleDeviceNuclearMuon(torch.optim.Optimizer):
    """
    NuclearMuon variant for usage in non-distributed settings.
    """
    def __init__(self, params, lr=0.02, weight_decay=0, momentum=0.95, nuclear_lambda=0.01):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nuclear_lambda=nuclear_lambda)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    # continue
                    p.grad = torch.zeros_like(p)  # Force synchronization
                state = self.state[p]
                if len(state) == 0:
                    state["momentum_buffer"] = torch.zeros_like(p)
                    state["step"] = 0
                state["step"] += 1

                # Apply layer-wise nuclear norm proximal operator to parameters
                if p.ndim >= 2:
                    step = state["step"]
                    adaptive_lambda = group["nuclear_lambda"] / (step**0.5)
                    p.data = prox_nuclear_norm(p.data, adaptive_lambda)

                update = nuclear_muon_update(p.grad, state["momentum_buffer"], beta=group["momentum"])
                p.mul_(1 - group["lr"] * group["weight_decay"])
                p.add_(update.reshape(p.shape), alpha=-group["lr"])

        return loss


def adam_update(grad, buf1, buf2, step, betas, eps):
    buf1.lerp_(grad, 1 - betas[0])
    buf2.lerp_(grad.square(), 1 - betas[1])
    buf1c = buf1 / (1 - betas[0]**step)
    buf2c = buf2 / (1 - betas[1]**step)
    return buf1c / (buf2c.sqrt() + eps)


class NuclearMuonWithAuxAdam(torch.optim.Optimizer):
    """
    Distributed NuclearMuon variant that can be used for all parameters in the network, since it runs an
    internal AdamW for the parameters that are not compatible with NuclearMuon. The user must manually
    specify which parameters shall be optimized with NuclearMuon and which with Adam by passing in a
    list of param_groups with the `use_nuclear_muon` flag set.

    The point of this class is to allow the user to have a single optimizer in their code, rather
    than having both a NuclearMuon and an Adam which each need to be stepped.

    Arguments:
        param_groups: List of parameter groups with `use_nuclear_muon` flag
    """
    def __init__(self, param_groups):
        for group in param_groups:
            assert "use_nuclear_muon" in group
            if group["use_nuclear_muon"]:
                group["params"] = sorted(group["params"], key=lambda x: x.size(), reverse=True)
                # defaults
                group["lr"] = group.get("lr", 0.02)
                group["momentum"] = group.get("momentum", 0.95)
                group["weight_decay"] = group.get("weight_decay", 0)
                group["nuclear_lambda"] = group.get("nuclear_lambda", 0.01)
                assert set(group.keys()) == set(["params", "lr", "momentum", "weight_decay", "nuclear_lambda", "use_nuclear_muon"])
            else:
                # defaults
                group["lr"] = group.get("lr", 3e-4)
                group["betas"] = group.get("betas", (0.9, 0.95))
                group["eps"] = group.get("eps", 1e-10)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(["params", "lr", "betas", "eps", "weight_decay", "use_nuclear_muon"])
        super().__init__(param_groups, dict())

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            if group["use_nuclear_muon"]:
                params = group["params"]
                if len(params) == 0:
                    continue
                params_pad = params + [torch.empty_like(params[-1])] * (dist.get_world_size() - len(params) % dist.get_world_size())
                for base_i in range(len(params))[::dist.get_world_size()]:
                    if base_i + dist.get_rank() < len(params):
                        p = params[base_i + dist.get_rank()]
                        if p.grad is None:
                            # continue
                            p.grad = torch.zeros_like(p)  # Force synchronization
                        state = self.state[p]
                        if len(state) == 0:
                            state["momentum_buffer"] = torch.zeros_like(p)
                            state["step"] = 0
                        state["step"] += 1

                        # Apply layer-wise nuclear norm proximal operator to parameters
                        if p.ndim >= 2:
                            step = state["step"]
                            adaptive_lambda = group["nuclear_lambda"] / (step**0.5)
                            p.data = prox_nuclear_norm(p.data, adaptive_lambda)

                        update = nuclear_muon_update(p.grad, state["momentum_buffer"], beta=group["momentum"])
                        p.mul_(1 - group["lr"] * group["weight_decay"])
                        p.add_(update.reshape(p.shape), alpha=-group["lr"])
                    dist.all_gather(params_pad[base_i:base_i + dist.get_world_size()], params_pad[base_i + dist.get_rank()])
            else:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["exp_avg"] = torch.zeros_like(p)
                        state["exp_avg_sq"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1
                    update = adam_update(p.grad, state["exp_avg"], state["exp_avg_sq"],
                                         state["step"], group["betas"], group["eps"])
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update, alpha=-group["lr"])

        return loss


class SingleDeviceNuclearMuonWithAuxAdam(torch.optim.Optimizer):
    """
    Non-distributed variant of NuclearMuonWithAuxAdam.
    """
    def __init__(self, param_groups):
        for group in param_groups:
            assert "use_nuclear_muon" in group
            if group["use_nuclear_muon"]:
                # defaults
                group["lr"] = group.get("lr", 0.02)
                group["momentum"] = group.get("momentum", 0.95)
                group["weight_decay"] = group.get("weight_decay", 0)
                group["nuclear_lambda"] = group.get("nuclear_lambda", 0.01)
                assert set(group.keys()) == set(["params", "lr", "momentum", "weight_decay", "nuclear_lambda", "use_nuclear_muon"])
            else:
                # defaults
                group["lr"] = group.get("lr", 3e-4)
                group["betas"] = group.get("betas", (0.9, 0.95))
                group["eps"] = group.get("eps", 1e-10)
                group["weight_decay"] = group.get("weight_decay", 0)
                assert set(group.keys()) == set(["params", "lr", "betas", "eps", "weight_decay", "use_nuclear_muon"])
        super().__init__(param_groups, dict())

    @torch.no_grad()
    def step(self, closure=None):

        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            if group["use_nuclear_muon"]:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1

                    # Apply layer-wise nuclear norm proximal operator to parameters
                    if p.ndim >= 2:
                        step = state["step"]
                        adaptive_lambda = group["nuclear_lambda"] / (step**0.5)
                        p.data = prox_nuclear_norm(p.data, adaptive_lambda)

                    update = nuclear_muon_update(p.grad, state["momentum_buffer"], beta=group["momentum"])
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update.reshape(p.shape), alpha=-group["lr"])
            else:
                for p in group["params"]:
                    if p.grad is None:
                        # continue
                        p.grad = torch.zeros_like(p)  # Force synchronization
                    state = self.state[p]
                    if len(state) == 0:
                        state["exp_avg"] = torch.zeros_like(p)
                        state["exp_avg_sq"] = torch.zeros_like(p)
                        state["step"] = 0
                    state["step"] += 1
                    update = adam_update(p.grad, state["exp_avg"], state["exp_avg_sq"],
                                         state["step"], group["betas"], group["eps"])
                    p.mul_(1 - group["lr"] * group["weight_decay"])
                    p.add_(update, alpha=-group["lr"])

        return loss
