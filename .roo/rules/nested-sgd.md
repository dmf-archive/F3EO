# 理论洞察：从嵌套 SGD 到原生稀疏更新 (Native Sparse Update)

**核心论点**: 虽然 Google 的 **Titans** 与 **RWKV-7 (Goose)** 通过 **嵌套 SGD (Nested SGD)** 范式极大地提升了模型的长程记忆与上下文学习能力，但这种“将状态压入固定矩阵”的 Fast Weight 模式仍存在本质局限。真正的持续学习 (Continual Learning) 应摆脱预定义的固定容量瓶颈，向**原生稀疏激活 (MoE)** 与**语义路由驱动的动态更新**演进。

## 1. 范式演进：Fast Weights 的崛起与瓶颈

### 1.1 优化的本质：推理即学习

Nested SGD 范式将模型的权重区分为静态的“外层权重” (Outer Weights) 与推理时动态更新的“内层权重” (Inner Weights)。这种演化本质上是将优化过程本身嵌入到了前向传播中。

### 1.2 Titans: 显式的测试时记忆 (Test-Time Memorization)

Titans 引入了 Neural Long-Term Memory (LMM)，其核心逻辑是在前向传播中通过梯度下降更新一组内层权重。

- **机制**: 利用“惊喜度” (Surprise) 驱动的动量 SGD。
- **形式化**:
  `S_t = beta * S_{t-1} + grad(Loss_associative)`
  `M_t = (1 - alpha) * M_{t-1} - eta * S_t`
- **本质**: 将序列历史编码进一个非线性的、可学习的权重空间。

### 1.3 RWKV-7: 解析式的动态状态演化

RWKV-7 通过广义 Delta Rule 实现了类似的效果，但更加轻量化。

- **机制**: 向量值门控 (Vector-valued gating) 与在序学习率 (In-context learning rates)。
- **形式化**:
  `S_t = G_t * S_{t-1} + v_t * k_hat_t^T`
  其中 `G_t` 包含衰减与替换逻辑，在数学上等价于一种近似的秩-1 SGD 更新。

## 2. 局限分析：为什么 Fast Weights 还不够？

无论是 Titans 的记忆矩阵还是 RWKV-7 的递归状态，本质上都是将无限的外部信息流“挤压”进一个**固定大小 (Fixed-size)** 的预定义 MLP 或矩阵中。

- **问题**: 随着上下文增长，系统必然面临严重的灾难性遗忘或信息熵饱和。

Nested SGD 往往涉及对整个内层权重的稠密更新 (Dense Update)。

- **问题**: 在处理特定领域的细分语义时，更新整个矩阵不仅浪费计算资源，还会导致不相关的旧记忆被错误覆盖。

## 3. 未来范式：原生稀疏更新 (Native Sparse Update)

理想的状态演化不应是“隐藏状态作为可学习权重”，而应是**模型权重即隐藏状态**。

通过引入细粒度专家混合 (MoE) 或动态稀疏架构，模型可以根据输入语义仅激活极小比例的参数。

- **Top-Any 动态激活**: 不再受限于固定的 Top-K，而是根据路由匹配度动态决定激活范围。

更新不再是盲目的梯度下降，而是基于语义路由的精准写入。

- **自然演化**: 只有与当前语义相关的“专家”或“神经元”会被更新，从而在数学上实现真正的零干扰持续学习。
- **形式化**: 更新量 `delta_theta` 仅在由路由算子 `R(x)` 定义的稀疏流形上非零。

## 4. 结论：向计算本体论回归

Nested SGD 是通往 AGI 的重要阶梯，但要实现真正的永续学习，我们必须从“在套娃里优化小娃”转向“让优化过程自适应选择需要优化的部分”。
