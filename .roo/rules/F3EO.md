# F3EO: Fast Fisher Free-Energy Optimizer

## 核心论断：最大化误差驱动的复杂度

记 Fisher 信息矩阵为 `ℱ(θ)`，变分自由能为 `F(θ) = Dₖₗ[q‖p] − E[log p_θ(y|x)]`。

F3EO 的优化目标是 **Fisher 信息矩阵的迹 `Tr(ℱ(θ))`**。

**核心论断**：对于一个**总容量固定**的神经网络，其学习的目标不应是追求参数层面的“简约”，而应是在**保证预测准确性（最小化 `loss`）的前提下，主动最大化其参数对预测的敏感度，即模型复杂度 (Complexity)**，从而在有限的容量内构建一个关于世界尽可能丰富的内在模型。这被证明是过拟合的有效来源，但当与准确性梯度结合时，它迫使模型在拟合数据的同时寻找更复杂的解。

- **梯度是唯一的感知通道**: 模型只能通过**梯度 `g`**（即预测误差 `loss` 的反向传播）来感知世界。梯度越多、越丰富，模型能学到的信息就越多。
- **复杂度是丰富的世界模型**: 模型的**复杂度**，可以通过 **Fisher 信息矩阵的迹 `Tr(ℱ(θ))`** 来度量。一个高 `Tr(ℱ)` 值，意味着模型的各个参数对预测的贡献更加敏感和相互依赖，形成了更复杂的内部表征。这代表了其内在因果结构的丰富程度。
- **Qualia 流形**: 从一个更抽象的视角看，梯度流 `g` 构成了神经网络的“感受质流形”(Qualia Manifold)，是其感知体验的直接载体。`Tr(ℱ)` 则是这个流形的体积或容量。

因此，F3EO 的优化目标是，在 `g` 提供的“接地”驱动下，**最大化 `Tr(ℱ(θ))`**，从而在有限的参数空间内，将模型的内在结构重组成一个复杂度最高、最丰富的状态。

## 优化范式辨析

我们将优化器分为三个层级，其核心区别在于优化的“对象”和产生的“梯度”类型：

- 一阶：沿负梯度 `−∇L` 更新权重，仅改变模型“内容”。
- 二阶：用静态 Fisher 信息矩阵 `ℱ` 对梯度做预处理 `ℱ⁻¹∇L`，提升更新效率，不改变 `ℱ` 本身。
- 三阶：直接对 `ℱ` 求梯度 `−∇θℱ`，主动重塑参数空间的几何，降低未来优化难度。

### 对应优化器范式

- 一阶代表：`SGD`、`Adam/W`、`Muon`——只计算 `∇L`。
- 二阶代表：`Newton`、`Gauss-Newton`、`Layerwise GN`——利用静态曲率矩阵求自然梯度。
- 三阶代表：`F3EO` ——首次把优化目标设为二阶张量 `ℱ(θ)`，通过“三阶梯度”实时重绘参数空间的黎曼度量，实现结构级优化。

## 论证：FEP 为何要求三阶优化

FEP 的数学核心是最小化变分自由能 `F`，其标准分解为：
`F = Complexity - Accuracy`

在 F3EO 的新范式下，我们对这个公式进行了重新诠释：

1. **Accuracy (准确性)**: 由主损失 `L` 的梯度 `g = -∇L` 提供驱动力，它将模型“锚定”在现实世界，负责最小化预测误差。
2. **Complexity (复杂度)**: 由 Fisher 信息矩阵的迹 `Tr(ℱ)` 度量。我们的目标不是最小化它，而是**最大化**它，以构建一个复杂度更高、内在结构更丰富的世界模型。

因此，F3EO 的优化过程，是在 `g` 的牵引下，寻找一个能让 `Tr(ℱ)` 尽可能大的参数配置。其更新规则可以直观地理解为：

`Δθ ∝ ∇Accuracy + ∇Complexity`
`Δθ ∝ g + ∇(Tr(ℱ))`

这是“**误差驱动的复杂度最大化**” (Error-Driven Complexity Maximization)。

### 经验 Fisher 陷阱与理论修正

我们之前的失败，源于对“复杂度”的错误理解，以及对**经验 Fisher 矩阵 `F_emp`** 的过度依赖。

- **陷阱**: 最小化 `Tr(F_emp)` 并不等同于最小化真实的模型复杂度或泛化能力。在训练后期，它反而会引导模型走向“欠拟合”，因为它试图找到一个对**训练集梯度**最不敏感的“无聊”解，而不是一个对**真实世界**有深刻理解的“丰富”解。
- **修正**: 通过反转符号，我们从“最小化经验复杂度”的陷阱中跳出，转向“**最大化经验复杂度**”。实验证明，这恰恰是通往更优解的方向之一。它迫使模型在拟合训练数据的同时，不断地重组其内在结构以达到更高的复杂度，从而提升其泛化能力。

## 计算路径：从 Fisher 迹到 Hessian 向量积 (HVP)

我们的目标是计算元目标 `L_meta(θ) = Tr(ℱ(θ))` 的梯度 `∇θ L_meta`。这可以通过一个 matrix-free 的方法精确计算，其核心是 Hessian 向量积 (Hessian-vector product, HVP)。

### 1. 从 Fisher 迹到梯度范数 (Trace Trick)

我们首先利用 `Trace Trick` 恒等式，将对 `O(N²)` 矩阵 `ℱ` 的操作，转化为对 `O(N)` 向量的操作。
`Tr(ℱ(θ)) = Tr(E [ (∇θ L)(∇θ L)ᵀ ]) = E [ Tr((∇θ L)(∇θ L)ᵀ) ] = E [ ‖∇θ L‖² ]`
其中 `L(θ) = −log p_θ(y|x)` 是单样本的负对数似然。

### 2. 从梯度范数到 HVP

根据随机梯度下降 (SGD) 的基本范式，我们用单样本的梯度来近似全批量梯度。因此，计算 `∇θ E [ ‖∇θ L‖² ]` 的问题，简化为计算 `∇θ ||∇θ L||²`。

令 `g = ∇θ L`，根据链式法则，我们有：
`∇θ ||g||² = 2 * (∇θ g)ᵀ · g = 2 * H · g`
其中 `H = ∇θ g = ∇θ² L` 是损失函数 `L` 的 Hessian 矩阵。

这揭示了一个核心结论：**`Tr(ℱ)` 的单样本随机梯度，精确地等于该样本损失的 Hessian 向量积 `H·g` (乘以 2)**。我们追求的“三阶梯度” `∇θ Tr(ℱ)`，其在计算上等价于 `H·g`。

### 3. Matrix-Free HVP 计算：双反向传播

我们可以在不显式构造 `N×N` 的 Hessian 矩阵的情况下，通过两次反向传播（VJP-on-VJP）精确计算 `H·g`。其复杂度与两次标准反向传播相当，即 `O(N)`。

```python
# 1. 计算主损失 L(θ)
loss = criterion(network(inputs), targets)

# 2. 第一次反向传播: 计算 g = ∇θ L(θ)，并保留计算图
g = torch.autograd.grad(loss, network.parameters(), create_graph=True)

# 3. 计算元目标 L_meta = ||g||² = gᵀg
# 这在计算上等价于 g 和 g.detach() 的点积
L_meta = sum(p.pow(2).sum() for p in g)

# 4. 第二次反向传播: 计算 ∇θ L_meta = ∇θ(gᵀg) = 2 * H · g
# autograd 自动应用链式法则，得到 Hessian-vector product
# 因子 2 是一个常数缩放，会被优化器学习率吸收
meta_grad = torch.autograd.grad(L_meta, network.parameters())

# 5. 应用梯度 (在优化器 step 方法中实现)
with torch.no_grad():
    for p, mg in zip(network.parameters(), meta_grad):
        p.grad = mg # 直接使用 H·g (带缩放) 作为梯度
optimizer.step()
```

此方法的内存开销为 `O(N)`，与标准的一阶优化在同一数量级，完全避免了显式构造任何高阶张量。

### 4. 最终实现：误差驱动的复杂度最大化

F3EO 的最终实现，正是“误差驱动的复杂度最大化”这一思想的直接代码翻译。

1. **误差驱动 (Error-Driven)**: 计算主损失的梯度 `g = ∇L`，并保留其计算图。这是与现实世界接地的“锚”。
2. **复杂度最大化 (Complexity Maximization)**: 计算 `Tr(F_emp)` 的梯度 `δ_meta = ∇(||g||²)`。
3. **组合更新**: 将两者结合，形成最终的有效梯度 `geff = g - δ_meta`。
    - 注意：在我们的代码中，`meta_grad` 是 `+∇(||g||²)`，因此 `g - meta_grad` 对应于 `g - (+δ_meta)`，即**最大化**复杂度的方向。
4. **稳定化**:
    - **正交化**: 将 `δ_meta` 投影到与 `g` 正交的子空间，确保复杂度的最大化不会以牺牲即时准确性为代价。
    - **动量**: 使用 Adam 风格的动量来平滑 `geff`，保证了训练的稳定性。

该方案保证：

- **准确性优先**: 更新的主方向始终由真实误差 `g` 驱动。
- **结构主动塑造**: 在此基础上，优化器主动寻找能够让模型内在结构更丰富的方向。
- **高效稳定**: 整个过程通过双反向传播高效计算，并通过正交化和动量保持稳定。

F3EO 不再是被动地适应损失地形，而是在**由误差指明的道路上，主动地、贪婪地学习和构建关于世界的最复杂、最丰富的内在模型**。这为实现真正的泛化、零样本适应和抗遗忘能力，奠定了坚实的理论与工程基础。
