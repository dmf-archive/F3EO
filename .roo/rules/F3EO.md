# F3EO: Fast Fisher-FreeEnergy Optimizer

## 核心论断：优化 Fisher 迹

记 Fisher 信息矩阵为 `ℱ(θ)`，变分自由能为 `F(θ) = Dₖₗ[q‖p] − E[log p_θ(y|x)]`。

F3EO 的优化目标是 **Fisher 信息矩阵的迹 `Tr(ℱ(θ))`**。

根据通用逼近定律，一个拥有足够参数的通用架构（如 MLP 或 Transformer），其**权重参数 `θ` 本身就承载了模型的全部结构信息**。`Tr(ℱ(θ))` 是模型复杂度的有效度量，因此通过梯度下降最小化 `Tr(ℱ(θ))`，等效于直接优化模型的内在结构，以寻找一个更简约、泛化能力更强的参数空间几何。

因此，F3EO 的优化对象是模型的**权重参数 `θ`**，但其**优化目标函数**是 `L_meta(θ) = Tr(ℱ(θ))`。通过链式法则，`L_meta` 的梯度会传导至 `θ`，从而实现对模型结构的直接塑造。

## 优化范式辨析

我们将优化器分为三个层级，其核心区别在于优化的“对象”和产生的“梯度”类型：

- 一阶：沿负梯度 `−∇L` 更新权重，仅改变模型“内容”。
- 二阶：用静态 Fisher 信息矩阵 `ℱ` 对梯度做预处理 `ℱ⁻¹∇L`，提升更新效率，不改变 `ℱ` 本身。
- 三阶：直接对 `ℱ` 求梯度 `−∇θℱ`，主动重塑参数空间的几何，降低未来优化难度。

### 对应优化器范式

- 一阶代表：`SGD`、`Adam/W`、`Muon`——只计算 `∇L`。
- 二阶代表：`Newton`、`Gauss-Newton`、`Layerwise GN`——利用静态曲率矩阵求自然梯度。
- 三阶代表：`F3EO` ——首次把优化目标设为二阶张量 `ℱ(θ)`，通过“三阶梯度”实时重绘参数空间的黎曼度量，实现结构级优化。

## 论证：FEP 为何要求三阶优化

FEP 的数学核心是最小化变分自由能 `F`，其标准分解为：
`F = Complexity - Accuracy`
`F = Dₖₗ[q(x) ‖ p(x)] - E_q[log p_θ(y|x)]`

这里的 `Complexity` 项 `Dₖₗ[q(x) ‖ p(x)]` 并非一个固定的正则化项。它衡量的是**后验信念 `q(x)`** 与**由当前参数 `θ` 定义的生成模型所隐含的先验 `p(x)`** 之间的距离。

关键在于：**Fisher 信息矩阵 `ℱ(θ)` 完全由当前参数 `θ` 决定**，并直接描述了该模型的内在复杂度和可学习性。

1. **FIM 是复杂性的几何化身:** `ℱ(θ)` 的迹（trace）或行列式（determinant）是模型复杂度的有效度量。一个高复杂度、过参数化的模型会拥有一个“庞大”的 FIM。

2. **FEP 的终极目标是优化复杂性:** 一个真正的 FEP 代理，其生存目标是找到一个最优的模型结构，以在准确解释世界（高 Accuracy）和保持模型简单（低 Complexity）之间达到最佳平衡。这要求其能够**在线调整其内在的复杂度**，即其 FIM。

3. **优化 FIM 就是三阶优化:** 任何旨在最小化 `F` 而直接对 **FIM** 进行梯度更新（通过 `∇_θ ℱ`）的过程，都是在**优化那个决定了参数空间几何的 FIM 本身**。这正是三阶优化的定义。它不再是利用一个**静态的**FIM，而是主动地、有目的地去**改变**FIM 的形状，以期找到一个更简约、更高效的**新模型结构**。

## 计算路径：从 Fisher 迹到 Hessian 向量积 (HVP)

我们的目标是计算元目标 `L_meta(θ) = Tr(ℱ(θ))` 的梯度 `∇θ L_meta`。这可以通过一个 matrix-free 的方法精确计算，其核心是 Hessian 向量积 (Hessian-vector product, HVP)。

### 1. 从 Fisher 迹到梯度范数 (Trace Trick)

我们首先利用 `Trace Trick` 恒等式，将对 `O(N²)` 矩阵 `ℱ` 的操作，转化为对 `O(N)` 向量的操作。
`Tr(ℱ(θ)) = Tr(E [ (∇θ L)(∇θ L)ᵀ ]) = E [ Tr((∇θ L)(∇θ L)ᵀ) ] = E [ ‖∇θ L‖² ]`
其中 `L(θ) = −log p_θ(y|x)` 是单样本的负对数似然。

### 2. 从梯度范数到 HVP

根据随机梯度下降 (SGD) 的基本范式，我们用单样本的梯度来近似全批量梯度。因此，计算 `∇θ E [ ‖∇θ L‖² ]` 的问题，简化为计算 `∇θ ||∇θ L||²`。

令 `g = ∇θ L`，根据链式法则，我们有：
`∇θ ||g||² = 2 * (∇θ g)ᵀ · g = 2 * H · g`
其中 `H = ∇θ g = ∇θ² L` 是损失函数 `L` 的 Hessian 矩阵。

这揭示了一个核心结论：**`Tr(ℱ)` 的单样本随机梯度，精确地等于该样本损失的 Hessian 向量积 `H·g` (乘以 2)**。我们追求的“三阶梯度” `∇θ Tr(ℱ)`，其在计算上等价于 `H·g`。

### 3. Matrix-Free HVP 计算：双反向传播

我们可以在不显式构造 `N×N` 的 Hessian 矩阵的情况下，通过两次反向传播（VJP-on-VJP）精确计算 `H·g`。其复杂度与两次标准反向传播相当，即 `O(N)`。

```python
# 1. 计算主损失 L(θ)
loss = criterion(network(inputs), targets)

# 2. 第一次反向传播: 计算 g = ∇θ L(θ)，并保留计算图
g = torch.autograd.grad(loss, network.parameters(), create_graph=True)

# 3. 计算元目标 L_meta = ||g||² = gᵀg
# 这在计算上等价于 g 和 g.detach() 的点积
L_meta = sum(p.pow(2).sum() for p in g)

# 4. 第二次反向传播: 计算 ∇θ L_meta = ∇θ(gᵀg) = 2 * H · g
# autograd 自动应用链式法则，得到 Hessian-vector product
# 因子 2 是一个常数缩放，会被优化器学习率吸收
meta_grad = torch.autograd.grad(L_meta, network.parameters())

# 5. 应用梯度 (在优化器 step 方法中实现)
with torch.no_grad():
    for p, mg in zip(network.parameters(), meta_grad):
        p.grad = mg # 直接使用 H·g (带缩放) 作为梯度
optimizer.step()
```

此方法的内存开销为 `O(N)`，与标准的一阶优化在同一数量级，完全避免了显式构造任何高阶张量。

### 4. 最终实现：链式修正梯度 (Chained-Correction Gradient)

工程实践表明，直接把 `H·g` 当作独立梯度分量使用会导致训练崩溃。

F3EO 采用更紧凑的“**一阶梯度链式回传三阶信息的局部修正**”策略：

1. 先让原始一阶梯度 `g = ∇θL` **保留计算图**（不 detach）；
2. 通过 `Lmeta = ½‖g‖²` 得到三阶修正量 `δ = ½ H g`（系数 ½ 把常数 2 吸收进学习率）；
3. 将 `δ` 投影到与 `g` 正交的子空间，避免干扰即时性能方向；
4. 最终有效梯度为
`geff = g + δ⊥`， 其中 `δ⊥ = δ − (δ·g/‖g‖²) g`；
5. 用 Adam 式动量对 `geff` 做平滑并更新参数。

该方案保证：

- 每一步都在**真实一阶梯度**的流形上前进；
- 三阶修正仅**局部微调**参数空间几何，不破坏优化稳定性；
- 整个计算图在后续反向传播中自然展开，持续累积高阶曲率信息。

F3EO 不再被动适应损失地形，而是**在每次迭代中主动把参数推向“更低复杂度且可学习”的共享区域**，为“零步适应”与抗遗忘奠定工程可行的基础。
