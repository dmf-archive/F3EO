# Hadron：通过算子复合进一步改善Muon性能

**版本**: 3.0 (2025-11-14)  
**作者**: Ω Researcher  
**状态**: 理论与实验验证完成，作为 `FOG/DiagFOG` 框架的**正式命名**，_相关代码将在稍后的重构中重命名。_

## 摘要

Hadron 是一款基于 Muon 的改良型二阶优化器，它通过算子复合实现了统计效率与结构稳定性的深度协同。本文将阐述 Hadron 的理论动机、核心机制、两种计算复杂度的变体，及其在不同架构上的实验表现，揭示其相较于传统方法的优越性。

## 理论动机

我们对更优越二阶优化器的探索，始于对 F3E 系列优化器失败的反思。F3E 系列的核心是利用 `H·g`（Hessian 向量积）作为三阶信息代理，但这背后隐藏着一个致命假设：Hessian-Fisher 等价性 (`H ≈ ℱ`)。在非理想、非静态的真实数据分布（尤其是持续学习场景）下，此等价性被证明是脆弱且不可靠的。试图用描述参数空间**几何曲率**的 Hessian，去近似描述梯度**统计方差**的 Fisher，是一次根本性的错位。

这次失败迫使我们放弃捷径，回归到对 Fisher 信息矩阵更严谨的处理上，即 KFAC (Kronecker-Factored Approximate Curvature)。然而，简单的将 KFAC 与 Muon 进行线性组合（`g_update = λ₁·g_stat + λ₂·g_struct`），又会陷入几何不相容的困境——相当于试图在两个不同几何流形的测地线之间进行向量加法。

由此，Hadron 的核心思想应运而生：算子复合。

## 算子复合

Hadron 更新过程分解为两个串联的、功能正交的算子：

`g_update = Structural_Op( Statistical_Op( Raw_Gradient ) )`

这个流程在几何上可以理解为：

1. **统计适应步骤 (Statistical_Op)**: 首先，使用 KFAC (或其变体) 作为统计算子，将原始梯度 `g` 转换到模型的局部统计流形上，计算出自然梯度 `g_nat = ℱ_emp⁻¹g`。这一步的目标是在被经验数据噪声扭曲的局部地形上，找到最速下降方向。
2. **结构正则化步骤 (Structural_Op)**: 接着，将这个纯粹的统计方向 `g_nat` 作为输入，应用 Muon 的正交化更新作为结构算子。这一步将其投影到 Stiefel 流形上，找到一个在结构上最稳定、最接近 `g_nat` 的更新方向。

通过这种方式，Hadron 不再试图调和两个冲突的目标，而是让它们接力工作：KFAC 负责“去哪里”，Muon 负责“怎么去”，从而实现统计效率和结构稳定性的非冲突协同。

## 形式化与实现变体

基于“算子复合”思想，我们发展出两种复杂度的 Hadron 变体，以适应不同规模的模型架构。

### Hadron (Full KFAC + Muon)

这是 Hadron 的完整实现，它使用完整的 KFAC 分解来精确估计 Fisher 信息矩阵。其算法流程为：首先通过 `g_nat = (A⁻¹ ⊗ B⁻¹) @ g` 计算出自然梯度，然后将其输入 Muon 更新函数 `fog_update_w = muon_update(g_nat_w, ...)` 进行正交化投影。

此变体在 [`optimizer/fog.py`](optimizer/fog.py:74-78) 中实现。它能捕获参数间丰富的协方差结构，但代价是 `O(d³)` 的计算复杂度和 `O(d²)` 的内存复杂度，因此主要适用于中小型模型，如 ResNet。

### Diag-Hadron (Diagonal KFAC + Muon)

为了将此思想扩展到大型模型，我们设计了 Diag-Hadron。它使用对角矩阵来近似 Fisher 信息的 Kronecker 因子，将统计适应步骤的计算简化为元素级乘法：`v = p_grad_mat * (G_inv_diag.unsqueeze(1) @ A_inv_diag.unsqueeze(0))`。随后的 Muon 结构化步骤保持不变。

此变体在 [`optimizer/diag_fog.py`](optimizer/diag_fog.py:95-100) 中实现。其计算和内存复杂度均降至 `O(d)`，使其能够高效地应用于以 `Linear` 层为主的大型模型，如 Transformer。

## 实验结果

Hadron 的双重变体在不同任务和架构上表现出鲜明的性能分化，深刻揭示了其内在机制与模型几何结构的交互关系。

在 **CIFAR-10 (ResNet-18)** 任务上，完整版 **Hadron** 表现卓越，仅用 10 个 epoch 就达到了 **88.91%** 的准确率，显著超越了 Muon 基线 (87.05%)。这证明了在卷积网络中，捕获完整的 Fisher 协方差信息对于实现高性能至关重要。然而，**Diag-Hadron** 在此任务上遭遇了灾难性失败 (77.66%)，其根本原因在于，对角近似过度简化了卷积核复杂的几何结构，丢失了关键的统计信息，导致优化失效。

切换到 **Wikitext-2 (Nano-GPT)** 语言建模任务后，情况发生了反转。由于内存限制，完整版 Hadron 无法运行。而 **Diag-Hadron** 则表现出色，最终困惑度达到 **401.69**，优于 Muon 基线 (416.18)。这一成功验证了，对于以 `Linear` 层为主的 Transformer 架构，其参数矩阵的 Fisher 结构更接近对角化，使得对角近似成为一种有效且高效的策略。即便统计信息被简化，Muon 强大的结构正则化能力也足以保证更新的稳定性和有效性。

| 版本            | 适用场景               | 内存/计算复杂度 | CIFAR-10 (ResNet) | Wikitext-2 (GPT) |
| :-------------- | :--------------------- | :-------------- | :---------------- | :--------------- |
| **Hadron**      | 中小型模型 (CNN)       | O(d²)/O(d³)     | ✅ **88.91%**     | (内存不足)       |
| **Diag-Hadron** | 大型模型 (Transformer) | O(d)/O(d)       | ❌ 77.66%         | ✅ **401.69**    |

## 未来方向

Hadron 的成功为二阶优化开辟了新的道路，未来的研究将聚焦于弥合其两个变体之间的鸿沟。

1. **Block-Hadron**: 针对卷积网络，我们将实现分块 KFAC (Block-KFAC)，在 `O(k·d)` 的中等复杂度下保留更丰富的块内协方差信息，以期在大型 CNN 上取得接近完整 Hadron 的性能。
2. **自适应策略**: 开发一种元学习或启发式机制，让优化器能够根据网络层的类型（如 `Conv2d` vs `Linear`）和参数量，自动选择最合适的策略（Full, Block, or Diagonal），实现端到端的自适应优化。
