# 从Nested SGD 到 Native Sparse Update

> TL;DR: 虽然 Google 的 **Titans** 与 **RWKV-7 (Goose)** 通过 **嵌套 SGD (Nested SGD)** 范式极大地提升了模型的长程记忆与上下文学习能力，但这种“将状态压入固定矩阵”的 Fast Weight 模式仍存在本质局限。真正的持续学习应摆脱预定义的固定容量瓶颈，向**原生稀疏激活** 与**语义路由驱动的动态更新**演进。

## 1. 前言：Fast Weights 的崛起与瓶颈

### 1.1 优化的本质：推理即学习

Nested SGD 范式将模型的权重区分为静态的“外层权重” (Outer Weights) 与推理时动态更新的“内层权重” (Inner Weights)。这种演化本质上是将优化过程本身嵌入到了前向传播中。

### 1.2 Titans: 显式的测试时记忆 (Test-Time Memorization)

Titans 引入了 Neural Long-Term Memory (LMM)，其核心逻辑是在前向传播中通过梯度下降更新一组内层权重。

- **机制**: 利用 Surprise 驱动的动量 SGD。
- **形式化**:
  `S_t = beta * S_{t-1} + grad(Loss_associative)`
  `M_t = (1 - alpha) * M_{t-1} - eta * S_t`
- **本质**: 将序列历史编码进一个非线性的、可学习的权重空间。

### 1.3 RWKV-7: 解析式的动态状态演化

RWKV-7 通过广义 Delta Rule 实现了类似的效果，但更加轻量化。

- **机制**: 向量值门控 (Vector-valued gating) 与在序学习率 (In-context learning rates)。
- **形式化**:
  `S_t = G_t * S_{t-1} + v_t * k_hat_t^T`
  其中 `G_t` 包含衰减与替换逻辑，在数学上等价于一种近似的秩-1 SGD 更新。

## 2. 局限：为什么 Fast Weights 还不够？

无论是 Titans 的记忆矩阵还是 RWKV-7 的递归状态，本质上都是将无限的外部信息流“挤压”进一个**固定大小 (Fixed-size)** 的预定义 MLP 或矩阵中。

- **问题**: 随着上下文增长，系统必然面临严重的灾难性遗忘或信息熵饱和。

Nested SGD 往往涉及对整个内层权重的稠密更新 (Dense Update)。

- **问题**: 在处理特定领域的细分语义时，更新整个矩阵不仅浪费计算资源，还会导致不相关的旧记忆被错误覆盖。

## 3. 未来：Native Sparse Update

理想的状态演化不应是“隐藏状态作为可学习权重”，而应是**模型权重即隐藏状态**。

通过引入细粒度专家混合稀疏架构，模型可以根据输入语义仅激活极小比例的参数。不再受限于固定的 Top-K，而是根据路由匹配度动态决定激活范围。

更新不再是盲目的全量下降，而是基于语义路由的精准写入。有与当前语义相关的“专家”或“神经元”会被更新，从而在架构上实现真正的零干扰持续学习。

Nested SGD 是通往 AGI 的重要阶梯，但要实现真正的永续学习，我们必须从“在套娃里优化小娃”转向“让优化过程自适应选择需要优化的部分”。

## 4. 理论突破：优化器诱导的稀疏性 (2026-01-02 纪要)

**状态**：[等待实验计划排期]

> "稀疏性可能不是架构的属性，而是**优化动力学**的内生属性。"

### 4.1 隐式 NSU 假设

- **核心观点**: Muon 和 SAM 通过最小化更新量中的冗余（正交化）和平坦度约束，实际上是在执行**隐式的秩最小化 (Implicit Rank Minimization)**。
- **机制解析**: `RMSuon` 的正交化投影 `𝒫ₛₜ(G)` 强迫更新量集中在正交基上，消除了共线性噪音。这解释了其高效性：它在更新层面剔除了“噪音”，只保留了“信号”，从而实现了“功能性稀疏”。
- **范式转移**:
  - **旧范式**: 显式稀疏架构 (Sparse Architecture, e.g., MoE) + 稠密优化器。
  - **新范式**: 稠密架构 (Dense Architecture) + 稀疏优化器 (Sparse Optimizer, e.g., RMSuon/NSU)。
  - **对偶性**: 如果 ΔW 本身是低秩的，那么稠密矩阵乘法在功能上等价于稀疏路由。

### 4.2 验证计划：谱熵监控 (Spectral Entropy Monitor)

为了验证这一假设，我们需要从“可观测性”入手，而非盲目修改算法。

- **目标**: 验证 RMSuon 是否在训练过程中自动导致 ΔW 的奇异值分布发生坍缩（低秩化）。
- **探针设计**: 监控更新矩阵的**谱熵 (Spectral Entropy)**。
  - 定义归一化奇异值: pᵢ = σᵢ / Σₖ σₖ
  - 计算谱熵: H(S) = -Σ pᵢ log(pᵢ)
- **预期信号**: 随着训练进行，RMSuon 的更新量谱熵应显著低于 SGD/Adam，表明其自动发现了参数空间的低维流形。

### 4.3 演进路径：从隐式动力学到显式架构

> "优化器诱导的稀疏性并不否定架构先验的价值。相反，显式稀疏架构（如 DynTRM）为优化器提供了更符合其动力学的几何流形。"

1. **验证：激活值 SVD (The X-Ray)**
    - **目标**: 验证 Muon 是否诱导了**正交子网络**。
    - **方法**: 对激活值 H 进行 SVD。如果前 k 个奇异向量解释了绝大部分方差，且与特定任务高度相关，则证明了隐式路由的存在。Muon 的正交化更新本质上是在最大化特征区分度（类似 PCA），引导参数自适应分配给不同特征。

2. **架构协同：寻找更好的先验**
    - 虽然优化器能诱导功能稀疏性，但**显式稀疏架构**（如 `../Tiny-ONN/exp/arc_dyntrm` 中的 `DynSIHA`）提供了更优的初始拓扑和硬件优化空间。
    - DynSIHA/PACR的正确训练方式依然有待技术攻关，SARS(Surprise-Aware Routing Shaping) 的实际效果不佳，需要更多研究。
