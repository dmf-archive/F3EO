[experiment]
tasks = ["wikitext2"]
seed = 42
device = "cuda"

[model]
arch = "nano_gpt"
vocabulary_size = 50257
sequence_length = 128
embedding_size = 256
num_heads = 4
num_layers = 4

[data]
batch_size = 32
num_workers = 0
tokenizer_path = "./data/tokenizers/wikitext2_bpe_50k.json"

[optimizer]
name = "AdamW"
lr = 1e-4
weight_decay = 0.01
betas = [0.9, 0.95]

[scheduler]
name = "cosine"
T_max = 60

[train]
epochs = 2
log_every = 10
ckpt_every = 1000