[experiment]
task = "wikitext2"
seed = 42
device = "cuda"

[model]
arch = "nano_gpt"
vocabulary_size = 40479  # 对齐AdaFisher的词汇表大小
embedding_size = 768
sequence_length = 256
num_heads = 12
num_layers = 4

[data]
batch_size = 8
num_workers = 4
tokenizer_path = "./data/wikitext2_tokenizer.json"

[optimizer]
name = "F3EO"
lr = 0.001       # 对齐AdaFisher的学习率
weight_decay = 5e-4  # 对齐AdaFisher

# [scheduler]
# name = "cosine"
# T_max = 10

[train]
epochs = 10
log_every = 10
ckpt_every = 2

[early_stop]
patience = 10
threshold = 1.0  # 1.0 perplexity improvement required