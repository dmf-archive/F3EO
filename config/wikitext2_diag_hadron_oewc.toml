[experiment]
tasks = ["wikitext2"]
seed = 42
device = "cuda"

[model]
arch = "nano_gpt"
vocabulary_size = 40479
embedding_size = 768
sequence_length = 255
num_heads = 12
num_layers = 4

[data]
batch_size = 8
num_workers = 0
tokenizer_path = "./data/wikitext2_tokenizer.json"

[optimizer]
name = "DiagHadronOEWC"
lr = 0.0001
momentum = 0.9
stat_decay = 0.95
lifelong_decay = 0.999
damping = 0.001
kl_clip = 0.001
weight_decay = 0.1
TCov = 10
TInv = 100
muon_momentum = 0.95

# AdamW parameters for non-hidden layers
adam_lr = 0.0001
adam_weight_decay = 0.1
adam_betas = [0.9, 0.95]

[train]
epochs = 10
log_every = 10
ckpt_every = 2